{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import gensim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook parses the data from the IEMOCAP data set. Three different data types are used: \n",
    "- audio features (precomputed with pyAudioAnalysis module) of sentenses\n",
    "- aligned transcripts\n",
    "- emotional ratings\n",
    "\n",
    "Corrensponding values are identified by a file name of an original sentence, e.g. 'Ses01F_script03_1_F009'. In the whole preprocessing pipeline these names are used as dictionary keys in all dicts. The complete list of files can be seen by calling .keys() method on a dictionary.\n",
    "\n",
    "There are 10039 numpy files and emotional assignments, but 10037 transcripts, so the further number of files is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILES\n",
    "Just reading lists of all files of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/karolina/Documents/GSOC/data/'\n",
    "# file names are *.waw_st.npy\n",
    "numpy_features = glob.glob(DATA_PATH+'IEMOCAP_full_release/Session*/sentences/wav/*/*st.npy')\n",
    "# file names are *.wdseg\n",
    "forced_alignments = glob.glob(DATA_PATH+'IEMOCAP_full_release/Session*/sentences/ForcedAlignment/Ses*/*.wdseg')\n",
    "# *.txt files have assignments for the whole session not sentences\n",
    "emo_evaluations = glob.glob(DATA_PATH+'IEMOCAP_full_release/Session*/dialog/EmoEvaluation/*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DICTS \n",
    "Creating dictionaries for each data type, always using the filename as a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emotions_names = {'ang': np.int32(0), 'dis':np.int32(1), 'fea':np.int32(2), 'fru':np.int32(3), \n",
    "                  'hap':np.int32(4), 'neu':np.int32(5), 'sad':np.int32(6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# audio features dict - assigning paths to file names *(without extension)\n",
    "features_dict = {}\n",
    "for p in numpy_features:\n",
    "    features_dict[p.split('/')[-1].split('.')[0]] = np.load(p)\n",
    "\n",
    "# transcript files dict - assigning paths to file names *(without extension)    \n",
    "transcripts_paths_dict = {}\n",
    "for p in forced_alignments:\n",
    "    transcripts_paths_dict[p.split('/')[-1].split('.')[0]] = p\n",
    "    \n",
    "# messy evaluation dict - assigning evaluations (grand average) - from summary files to file names\n",
    "file = emo_evaluations[1]\n",
    "mean_emo_eval_dict = {}\n",
    "emotional_eval_dict = {}\n",
    "\n",
    "DATA_FLAG = False\n",
    "values = []     # this is an empty value needed fot the loop\n",
    "key = ''        # this as well\n",
    "for file in emo_evaluations:\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            split = line.split('\\t')\n",
    "            #print(split)\n",
    "            if len(split)>1:\n",
    "                split2 = split[1].split(';')\n",
    "                if len(split2)==1:\n",
    "                    if len(values)!=0:\n",
    "                        #print('saving',key,values)\n",
    "                        real_values = np.array([float(re.sub(\"[^0-9.]\",\" \", v)) for v in values.split(' ')], dtype=np.float32)\n",
    "                        mean_emo_eval_dict[key]=real_values\n",
    "                        if emo_tag in emotions_names:\n",
    "                            emotional_eval_dict[key] = emotions_names[emo_tag]\n",
    "                    key = split2[0]\n",
    "                    emo_tag = split[2]\n",
    "                    values = split[3]\n",
    "#                     # this part is for extracting the more detailed info, but that's omitted at the moment\n",
    "#                     values = []\n",
    "#                 if len(split2)>1:\n",
    "#                     values.append(split2)\n",
    "real_values = np.array([float(re.sub(\"[^0-9.]\",\" \", v)) for v in values.split(' ')], dtype=np.float32)\n",
    "mean_emo_eval_dict[key]=real_values #appending the last evaluation\n",
    "if emo_tag in emotions_names:\n",
    "    emotional_eval_dict[key] = emotions_names[emo_tag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD2VEC embedding\n",
    "\n",
    "Binary from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load_word2vec_format(DATA_PATH+'GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READING THE TRANSCRIPT & CORREESPONDING EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of sentences is not longer than 18 words, so the rest of the sentences is rejected to avoid extra padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_transcript(path):\n",
    "    \"\"\"\n",
    "    Reads and cleans the transcript from the forced-alignment files.\n",
    "    Cleans the 'non-words'\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path,delim_whitespace=True, usecols=['SFrm','EFrm','Word']).dropna()\n",
    "    df = df[~df.Word.str.contains('<')]\n",
    "    df = df[~df.Word.str.contains('\"')]\n",
    "    df = df[~df.Word.str.contains('LAUGHTER')]\n",
    "    df = df[~df.Word.str.contains('BREATHING')]\n",
    "    df = df[~df.Word.str.contains('GARBAGE')]\n",
    "    n_rows = df.shape[0]\n",
    "    return df, n_rows\n",
    "\n",
    "def clean_word(word):\n",
    "    return re.sub(\"[^a-zA-Z']\",\" \", word).lower()\n",
    "\n",
    "def pad_feature(feature,audio_start,audio_end):\n",
    "    empty_feature = np.zeros((34,240), dtype=np.float32) #all features have this size\n",
    "    empty_feature[:,:audio_end-audio_start] = feature[:,audio_start:audio_end]\n",
    "    audio_feature = empty_feature[:,:,None]\n",
    "    return audio_feature\n",
    "\n",
    "def pad_feature_sequence(feature_sequence):\n",
    "    sequence = np.zeros((34,240,18),dtype=np.float32)\n",
    "    sequence[:,:,:feature_sequence.shape[2]] = feature_sequence\n",
    "    return sequence\n",
    "\n",
    "def pad_word_sequence(word_sequence):\n",
    "    sequence = np.zeros((300,18),dtype=np.float32)\n",
    "    sequence[:,:word_sequence.shape[1]] = word_sequence\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ready_audio_dict = {}\n",
    "features_len_dict = {}\n",
    "ready_word_embed_dict = {}\n",
    "words_dict = {}\n",
    "\n",
    "ALL_KEYS = emotional_eval_dict.keys() #transcripts_paths_dict.keys()\n",
    "for key in list(ALL_KEYS):\n",
    "    features = features_dict[key]\n",
    "    transcript, n_rows = read_transcript(transcripts_paths_dict[key])\n",
    "    OUTPUT_FLAG = False\n",
    "    for n in range(n_rows):\n",
    "        w = transcript.Word.iloc[n]\n",
    "        lower = clean_word(w) #cleaning up the transcript\n",
    "        \n",
    "        if lower in model.vocab:\n",
    "            audio_start = int(transcript.SFrm.iloc[n])\n",
    "            audio_end = int(transcript.EFrm.iloc[n])\n",
    "            feature = features_dict[key]\n",
    "            audio_feature = pad_feature(feature,audio_start,audio_end)\n",
    "\n",
    "            word_embed = model[lower][:,None]\n",
    "            if OUTPUT_FLAG:\n",
    "                audio = np.concatenate((audio,audio_feature),axis=2)\n",
    "                word = np.concatenate((word,word_embed),axis=1)\n",
    "                all_words.append(lower)\n",
    "            else:\n",
    "                audio = audio_feature\n",
    "                word = word_embed\n",
    "                all_words = [lower]\n",
    "                OUTPUT_FLAG = True\n",
    "    if word.shape[1] < 19:          \n",
    "        ready_audio_dict[key] = pad_feature_sequence(audio)\n",
    "        ready_word_embed_dict[key] = pad_word_sequence(word)\n",
    "        features_len_dict[key] = np.int32(audio_end-audio_start)\n",
    "        words_dict[key] = all_words\n",
    "del model\n",
    "GOOD_KEYS = ready_audio_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ses03M_script02_1_M017'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(GOOD_KEYS)[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'my']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_dict[list(GOOD_KEYS)[8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DIVIDING DATA INTO SPLITS WITH EVENLY DISTRIBUTED SPEAKERS AND SCENES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The audio files (keys) are separatred depending on the condition they belong to. The number of splits is determined by the minimal numer of files from one condition, here 8.\n",
    "These splits can be used e.g. for the crossvalidation.\n",
    "In most of the cases there is only one realisation of a particular label for a given setup, so this is excluded from the mixing procedure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hash_dictionary = {}\n",
    "for sess_name in list(GOOD_KEYS):\n",
    "    #label = str(ready_audio_dict[sess_name])\n",
    "    split = sess_name.split('_')\n",
    "    ses_num = split[0][3:5]\n",
    "    ses_g = split[0][-1]\n",
    "    scenario = re.sub(\"[^a-z]\",\" \", split[1]).strip(' \\t\\n\\r')\n",
    "    scenario_num = re.sub(\"[^0-9]\",\" \", split[1]).strip(' \\t\\n\\r')\n",
    "    if len(split)==4:\n",
    "        division_num = split[2]\n",
    "        g = split[3][0]\n",
    "    else:\n",
    "        division_num = '0'\n",
    "        g = split[2][0]\n",
    "    hash_key = ses_num+ses_g+scenario+scenario_num+division_num+g#+label\n",
    "    if hash_key in hash_dictionary:\n",
    "        hash_dictionary[hash_key].append(sess_name)\n",
    "    else:\n",
    "        hash_dictionary[hash_key] = [sess_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_splits = 8\n",
    "splits = np.empty(n_splits,dtype=object) #numpy array with splitted data names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EMPTY_FLAG = True\n",
    "for k, v in hash_dictionary.items():\n",
    "    a = np.arange(len(v))\n",
    "    shuffle(v)\n",
    "    if EMPTY_FLAG:\n",
    "        for i in range(n_splits):\n",
    "            splits[i] = list(np.array(v)[a%n_splits==i])\n",
    "        EMPTY_FLAG = False\n",
    "    else:\n",
    "        for i in range(n_splits):\n",
    "            splits[i].extend(list(np.array(v)[a%n_splits==i]))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6031"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([len(s) for s in splits])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRITING TO BINARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BINARIES_PATH = DATA_PATH+'audio_features/IEMOCUP/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j,split in enumerate(list(splits)):\n",
    "    for i,key in enumerate(split):\n",
    "\n",
    "        # dividing into files\n",
    "        if i%100==0:\n",
    "            train_filename = BINARIES_PATH+'split_'+str(j)+'_'+str(i//100)+'.tfrecords'\n",
    "\n",
    "        writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "        \n",
    "        audio = ready_audio_dict[key].tobytes()\n",
    "        audio_len = features_len_dict[key].tobytes()\n",
    "        word = ready_word_embed_dict[key].tobytes()\n",
    "        emo = mean_emo_eval_dict[key].tobytes()\n",
    "        label = emotional_eval_dict[key].tobytes()\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'audio_features'    : tf.train.Feature(bytes_list=tf.train.BytesList(value=[audio])),\n",
    "            'audio_len'         : tf.train.Feature(bytes_list=tf.train.BytesList(value=[audio_len])),\n",
    "            'word_embeddings'   : tf.train.Feature(bytes_list=tf.train.BytesList(value=[word])),\n",
    "            'y'                 : tf.train.Feature(bytes_list=tf.train.BytesList(value=[emo])),\n",
    "            'label'             : tf.train.Feature(bytes_list=tf.train.BytesList(value=[label])),\n",
    "            }))\n",
    "\n",
    "        writer.write(example.SerializeToString())\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
